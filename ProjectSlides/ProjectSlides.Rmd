---
title: "Analyzing Book <br> Checkout Trends at the <br> Seattle Public Library"
author: "Sarvesh Fotedar, Ekrem Kizilkaya, Abbas Shaikh, Cody VanZandt, Andy Wang"
output: ioslides_presentation
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(RSQLite)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggbreak)
library(scales)
library(grid)
library(topicmodels)
library(tm)
library(Matrix)
library(wordcloud)
library(SnowballC)
library(Rtsne)
library(pheatmap)
library(gridExtra)

setwd('..')
source("useful_utils.R")
checkout_data <- get_checkouts(2018:2022)
dcon <- dbConnect(SQLite(), dbname = "data/SQLData.db")
```

## About the Datasets

-   **Seattle Public Library Checkouts**: \~42 million observations from 2005 to 2022
    -   Medium (physical, audio-book, CD, etc)
    -   Title, Subjects
    -   Publisher
-   **GoodReads Reviews**: \~15 million reviews for 2 million books from 2013 to 2017
    -   Rating
    -   Review Text
    
# Exploratory Data Analysis

## Number of Checkouts by Month
<center>
```{r 1, results='hide', fig.width=8, fig.height=5}
query <- paste0("
SELECT CheckoutMonth as month, CheckoutYear as year, SUM(Checkouts) as total
FROM checkouts
GROUP BY CheckoutMonth, CheckoutYear
ORDER BY CheckoutYear, CheckoutMonth ASC;
")
{
  res <- dbSendQuery(dcon, query)
  df <- dbFetch(res, n = -1)
  dbClearResult(res)
  remove(query)
}
df$date <- as.Date(with(df, sprintf("%d-%02d-01", year, month)),
                   format = "%Y-%m-%d")

ggplot(df, aes(x=date, y=total)) + 
  geom_line() +
  labs(
    x = "\nDate",
    y = "Checkouts\n"
  ) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_y_continuous(labels = label_number(suffix = "K", scale = 1e-3)) +
  theme_bw() +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14))
```
</center>

## Monthly Book, Ebook, and Audiobook Checkouts
<center>
```{r, results='hide', fig.width=8, fig.height=5}
query <- paste0("
SELECT CheckoutYear, CheckoutMonth, MaterialType, Publisher, SUM(Checkouts) as CheckoutSum
FROM checkouts
WHERE MaterialType = 'BOOK' OR MaterialType = 'EBOOK' OR MaterialType = 'AUDIOBOOK'
GROUP BY CheckoutYear, CheckoutMonth, MaterialType
ORDER BY CheckoutYear, CheckoutMonth, MaterialType
;")

res <- dbSendQuery(dcon, query)
bookFormatData <- dbFetch(res, -1)
dbClearResult(res)

bookFormatData$Date <- with(bookFormatData, sprintf("%d-%02d-01", CheckoutYear, CheckoutMonth))
bookFormatData$Date <- as.Date(bookFormatData$Date, format = "%Y-%m-%d")

ggplot(bookFormatData) +
  geom_line(mapping=aes(x=Date, y=CheckoutSum, color = MaterialType)) +
  labs(
    title = "Monthy checkouts of book, e-book, and audiobook formats from 2005-2022",
    x = "Date",
    y = "Monthly Checkouts"
  )
```
</center>

## Number of Checkouts by Medium
<center>
```{r 2, results='hide', fig.width=8, fig.height=5}
# Get Checkouts by Material Type
mat_types <- data.frame(table(checkout_data$MaterialType))
colnames(mat_types) <- c("MaterialType", "NumCheckouts")

# Sort in descending order
mat_types <- mat_types[with(mat_types,order(-NumCheckouts)),]

# Get Usage Class for each Material Type
usage_class = c()
for (type in mat_types$MaterialType){
  usage_class[type] = checkout_data$UsageClass[checkout_data$MaterialType == type][1]
}
mat_types["UsageClass"] = as.factor(usage_class)

# Add OTHER category as sum of all but first 15 columns, so that bar chart has top 15 material types and everything else under OTHER
levels(mat_types$MaterialType) <- c(levels(mat_types$MaterialType), 'OTHER')
levels(mat_types$UsageClass) <- c(levels(mat_types$UsageClass), 'Both')
mat_types[16,] = c('OTHER', sum(mat_types$NumCheckouts[16:nrow(mat_types)]), 'Both')

# Drop extra columns, resolve data types
mat_types <- mat_types[-c(17:nrow(mat_types)),]
mat_types$NumCheckouts <- as.numeric(mat_types$NumCheckouts)

ggplot(mat_types) + 
  aes(x = factor(MaterialType, level = mat_types$MaterialType), y = NumCheckouts, fill = factor(UsageClass, level = c("Physical", "Digital", "Both"))) +
  geom_col() + 
  theme(text = element_text(size=8), axis.text.x=element_text(angle = -90, vjust = 0.5, hjust=0)) +
  labs(title = "Number of Checkouts by Material Type from 2018 to 2022", x = "Material Type", y = "Number of Checkouts", fill = "Usage Class") +
  scale_y_cut(breaks=c(50000), which=c(1, 2), scales=c(5, 1), space = 0.02) +
  scale_x_discrete(labels = function(x) wrap.labels(x, 10))
```
</center>

## Most Popular Subjects

<center>
```{r 3, results='hide', fig.width=8, fig.height=5}
bars <- table(unlist(strsplit(checkout_data$Subjects, ", ")))
bars <- bars[order(-bars)][1:7]
pie(bars[1:7], main="Top 7 Checked-Out Book Subjects, 2018-2022",
    labels=names(bars),
    col=c('coral1', 'lightsalmon', 'moccasin', 'palegreen',
          'lightskyblue', 'mediumorchid3', 'hotpink1'))
```
</center>

## Most Popular Books

<center>
```{r 4, results='hide', fig.width=8, fig.height=5}
is_book <- checkout_data[["MaterialType"]] == "BOOK"
book_checkouts <- checkout_data[is_book,c("Title", "CheckoutYear", "Checkouts")]

all_years_books <- book_checkouts[ 
  book_checkouts$Title %in% book_checkouts[book_checkouts$CheckoutYear == 2018, "Title"] &
    book_checkouts$Title %in% book_checkouts[book_checkouts$CheckoutYear == 2019, "Title"] &
    book_checkouts$Title %in% book_checkouts[book_checkouts$CheckoutYear == 2020, "Title"] &
    book_checkouts$Title %in% book_checkouts[book_checkouts$CheckoutYear == 2021, "Title"] &
    book_checkouts$Title %in% book_checkouts[book_checkouts$CheckoutYear == 2022, "Title"],
]

title_counts <-aggregate(
  all_years_books$Checkouts,
  list(title=all_years_books$Title),
  FUN=sum
)

top_books <- book_checkouts[
  book_checkouts$Title %in% title_counts[
    order(title_counts$x, decreasing=TRUE),
  ][1:7,"title"],
]

top_books_counts <- aggregate(
  top_books$Checkouts,
  list(year=top_books$CheckoutYear, title=top_books$Title),
  FUN=sum
)

book_positions_by_year <- top_books_counts %>%
  group_by(year) %>%
  mutate(rank = rank(-x))%>%
  arrange(year, rank)

get_ranks_by_year <- function(title){
  positions <- book_positions_by_year[order(book_positions_by_year$year),]
  positions[positions$title == title, "rank"]$rank
}

titles <- book_positions_by_year[
  book_positions_by_year$year == 2018, "title"][["title"]]
clean_titles <- wrap.labels(sapply(strsplit(titles," / "), `[`, 1), 16) 

colors <- palette.colors(n=7)

par(las=1, mar=c(5.1, 6.4, 4.1, 2.1 ))
popular_plot <- plot(NULL, type="l", col="black", lwd=1.0,
     main="Most Popular Books 2018-2022", xlab="Year", ylab = "", yaxt='n',
     xlim=c(2018,2022), ylim=c(max(book_positions_by_year$rank),min(book_positions_by_year$rank)))


for(i in seq_along(titles)){
  title <- titles[[i]]
  ranks <- get_ranks_by_year(title)
  lines(2018:2022, ranks, col=colors[[i]])
  axis(2, at=i, labels=clean_titles[[i]], col.axis = colors[[i]])
}
```
</center>

# Data-Driven Insights

## Guiding Questions

1.  Why do some books enjoy widespread acclaim while others fizzle?
2.  Do popular books explode onto the scene or accumulate readers more gradually?
3.  How do the popular success and critical acclaim of novels differ?
4.  How has recent conglomeration of publishers changed the literary marketplace?
5.  And, ultimately, to what degree can the success or failure of a book be predicted?

## Reading Lines

>- Normal People: a Novel

>- Circe: a Novel

## Are Reading Lines More Popular?

<center>
```{r, results='hide', warning=FALSE, message=FALSE, fig.width=8, fig.height=5}
query <- paste0("
SELECT *
FROM checkouts
WHERE CheckoutYear >= 2018 AND CheckoutYear <= 2022
")
{
  res <- dbSendQuery(dcon, query)
  checkouts <- dbFetch(res, n = -1)
  dbClearResult(res)
  remove(query)
}

custom_clean_strings <- function(x){
  x <- sub("\\s?\\/\\s?.*$", "", x)
  x <- sub("\\[.*\\]", "", x)
  tolower(x)
}

has_reading_line <- function(x, line="novel"){
  str_detect(x, paste0(":\\s?(a|A)\\s",line))
}

strip_reading_line <- function(x){
  str_split_i(x, "\\s?:\\s", 1)
}

limit_text <- function(x, words=2){
  x <- str_split_1(x, ",")[1]
  x <- str_replace_all(x, "\\W+", " ")
  x <- str_split_1(x, "\\s")
  paste(x[1:min(words, length(x))], collapse=" ")
}


checkouts <- checkouts %>%
  filter(MaterialType %in% c("EBOOK", "AUDIOBOOK", "BOOK") ) %>%
  mutate(Title = custom_clean_strings(Title),
         Date = as.Date(paste0(CheckoutYear,"-",CheckoutMonth,"-01"), "%Y-%m-%d"),
         reading_line = has_reading_line(Title))

word_of_interest <- "novel"

books_with_tag <- checkouts %>%
  mutate(id_checkouts = row_number()) %>%
  filter( has_reading_line(Title) ) %>%
  group_by(Title) %>%
  summarize(n=sum(Checkouts)) %>%
  arrange(desc(n)) %>%
  pull(unique(Title))

books_without_tag <- unique(strip_reading_line(books_with_tag))

both <- checkouts %>%
  filter(strip_reading_line(Title) %in% books_without_tag ) %>%
  mutate(reading_line = has_reading_line(Title)) %>%
  group_by(reading_line) %>%
  mutate( id =ifelse(reading_line == TRUE, row_number(), NA)) %>%
  ungroup()

both$reading_line<- factor(both$reading_line, levels=c(TRUE, FALSE))

# Books
both %>%
  filter(MaterialType == "BOOK") %>%
  group_by(reading_line, Date) %>%
  summarize(checkouts = mean(Checkouts)) %>%
  ggplot() +
  geom_smooth(aes(Date, checkouts, color=reading_line)) +
  ggtitle("Novel Popularity by Reading Line") +
  xlab("Time") +
  ylab("Mean Checkouts per Book per Month") +
  scale_color_manual(values=c("blue", "red")) +
  labs(color = "Reading Line")

```
</center>

## Where to Publish

<center>
```{r, results='hide', warning=FALSE, message=FALSE, fig.width=8, fig.height=5}

literary_publishers <- checkouts %>%
  filter(MaterialType == "BOOK") %>%
  group_by(Publisher ) %>%
  add_count() %>%
  filter(n > 1000) %>%
  ungroup() %>%
  group_by(Publisher) %>% 
  summarise(reading_line = mean(reading_line), quality=mean(Checkouts)) %>%
  arrange(desc(reading_line)) %>%
  filter(reading_line > .05)


best_literary_publishers <- literary_publishers %>%
  filter(reading_line >= .2*sd(reading_line) + mean(reading_line) & quality >= .20*sd(quality) + mean(quality)) %>%
  mutate(publisher = unname(sapply(Publisher, limit_text)))

ggplot(literary_publishers,aes(reading_line, quality)) +
  geom_point() +
  geom_point(data=best_literary_publishers,color="red")+
  scale_x_log10()+
  scale_y_log10()+
  ggtitle("Finding a Quality Literary Publisher")+
  xlab("Literariness") +
  ylab("Quality")
```
</center>

## (The Best of) Where to Publish

<center>
```{r, results='hide', warning=FALSE, message=FALSE, fig.width=8, fig.height=5}

ggplot(best_literary_publishers,aes(reading_line, quality)) +
  geom_point(data=best_literary_publishers,color="red")+
  geom_text(aes(label=publisher), size=3, vjust=1.5) +
  scale_x_log10()+
  scale_y_log10()+
  ggtitle("The Best Literary Publishers")+
  xlab("Literariness") +
  ylab("Quality")
```
</center>

## Controversy

Controversy score is defined as $\frac{pn}{|p-n|}$,
where $p$ is the number of 5-star reviews and $n$ is the number of 1-star reviews

## Controversial Books

```{r, warning=FALSE, message=FALSE}
setwd('..')
controversial_books <- read_csv("./data/controversial_titles.csv")
controversial_books %>% 
  mutate( c = c - mean(c)) %>%
  mutate(title = reorder(title, c), c_color = ifelse(c>0,"red", "blue")) %>%
  ggplot(aes(title, c, fill = c_color)) +
  geom_col() +
  coord_flip() + 
  labs(y = "Controversy Score", x="Title", title="Most and Least Controversial Texts")
```

## Controversial Words

```{r, warning=FALSE, message=FALSE}
setwd('..')
controversial_words <- read_csv("./data/controversial_words.csv")

slice_tail(controversial_words, n=-2) %>% 
  filter(row_number() > max(row_number()) - 20 | row_number() <= 20) %>% 
  mutate(
    term = reorder(sub("tfidf_review_text_", "", term), estimate),
    term_color = ifelse(estimate>0,"controversial", "uncontroversial"),
    ) %>%
  ggplot(aes(term, estimate, fill = term_color)) +
  geom_col() +
  coord_flip() + 
  labs(y = "Controversy Score", x="Title", title="Controversial Review Language",
       fill = ""
       )
```

## Publisher Conglomeration

<center>
```{r, results='hide', warning=FALSE, message=FALSE, fig.width=8, fig.height=5}
query <- paste0("
SELECT Publisher, CheckoutYear, CheckoutMonth, SUM(Checkouts) as CheckoutSum, MaterialType
FROM checkouts
WHERE MaterialType = 'BOOK' OR MaterialType = 'EBOOK' OR MaterialType = 'AUDIOBOOK'
GROUP BY Publisher, CheckoutYear, CheckoutMonth, MaterialType
ORDER BY CheckoutSum
;")

res <- dbSendQuery(dcon, query)
publishers <- dbFetch(res, -1)
dbClearResult(res)

publishers$Date <- with(publishers, sprintf("%d-%02d-01", CheckoutYear, CheckoutMonth))
publishers$Date <- as.Date(publishers$Date, format = "%Y-%m-%d")

totalPublishers <- publishers %>% 
  group_by(Date) %>% 
  summarise(sum = sum(CheckoutSum))

totalPublishers$Proportion <- NA

for (date in totalPublishers$Date){
  dateSubset <- publishers[publishers$Date == date,]
  topPublishers <- tail(dateSubset$Publisher, 0.05 * length(dateSubset$Publisher))
  totalPublishers$Proportion[totalPublishers$Date == date] <- sum(dateSubset$CheckoutSum[dateSubset$Publisher %in% topPublishers]) / totalPublishers$sum[totalPublishers$Date == date]
}

sumPublishers <- publishers %>% 
  group_by(Date, MaterialType) %>% 
  summarise(TotalCheckouts = sum(CheckoutSum))

sumPublishers$Proportion <- NA

for (format in c("BOOK", "EBOOK", "AUDIOBOOK")){
  dataSubset <- publishers[publishers$MaterialType == format,]
  for (date in sumPublishers$Date[sumPublishers$MaterialType == format]){
    dateSubset <- dataSubset[dataSubset$Date == date,]
    topPublishers <- tail(dateSubset$Publisher, 0.05 * length(dateSubset$Publisher))
    sumPublishers$Proportion[sumPublishers$Date == date & sumPublishers$MaterialType == format] <- sum(dateSubset$CheckoutSum[dateSubset$Publisher %in% topPublishers]) / sumPublishers$TotalCheckouts[sumPublishers$Date == date & sumPublishers$MaterialType == format]
  }
}

plot1 <- ggplot(totalPublishers) +
  geom_line(mapping=aes(x=Date, y=Proportion)) +
  labs(
    title = "Proportion of Checkouts from Top 5% of All Publishers",
    x = "Date",
    y = "Proportion of Montly Checkouts"
  ) + 
  scale_y_continuous(limits=c(0.75,1))

plot2 <- ggplot(sumPublishers) +
  geom_line(mapping=aes(x=Date, y=Proportion, color = MaterialType)) +
  labs(
    title = "Proportion of Checkouts from Top 5% of Publishers by Format",
    x = "Date",
    y = "Proportion of Montly Checkouts"
  ) + 
  scale_y_continuous(limits=c(0.1, 1))

grid.arrange(plot1, plot2, ncol=2)
```
</center>

# Secondary Dataset

## Sentiment Analysis
<center>
```{r, warning=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=5}
setwd("..")

# setwd("/home/ekrem/Documents/classes/stat405/STAT405-605/")

sent_rat <- readr::read_csv("./data/sent_rating.csv")
ggplot(sent_rat, aes(x = score-rating)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white") +
  labs(x = "Difference", y = "Frequency", title = "Difference Between Rating and Sentiment Score")
```
</center>

## Word Cloud

<center>
```{r, warning=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=5}
setwd("..")

cleaned_docs <- read.csv("./data/cleaned.csv")

corpus <- Corpus(VectorSource(cleaned_docs$text))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)

# Convert the term-document matrix to a matrix
matrix <- as.matrix(tdm)

# Calculate the frequency of words
word_freqs <- sort(rowSums(matrix), decreasing = TRUE)

# Create a data frame with words and their frequencies
word_freqs_df <- data.frame(Word = names(word_freqs), Freq = word_freqs)

# Create the word cloud
wordcloud(words = word_freqs_df$Word, freq = word_freqs_df$Freq, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, "Dark2"))

```
</center>

## Genre Similarity

<center>
```{r, warning=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=5}
num_topics <- 10
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))

theta <- posterior(lda_model)$topics

tsne_result <- Rtsne(theta, perplexity = 2, check_duplicates = FALSE)

# Create a data frame with the t-SNE results
tsne_data <- data.frame(tsne_x = tsne_result$Y[, 1], tsne_y = tsne_result$Y[, 2])

doc_labels <- cleaned_docs$genre

# Create a data frame with the t-SNE results and labels
tsne_data <- data.frame(tsne_x = tsne_result$Y[, 1], tsne_y = tsne_result$Y[, 2], label = doc_labels)

# Create a scatterplot of the t-SNE results with labels
ggplot(tsne_data, aes(x = tsne_x, y = tsne_y, label = label)) +
  geom_point(size = 4, color = "blue") +
  geom_text(aes(label = label), vjust = 1, hjust = 1, size = 3, color = "black") +
  ggtitle("t-SNE Plot of Genre Based on LDA Topic Distributions") +
  xlab("t-SNE Dimension 1") +
  ylab("t-SNE Dimension 2")



```
</center>

## Genre Similarity Matrix

<center>
```{r, warning=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=5}
library(proxy)

# Calculate the topic distributions for each document
theta <- posterior(lda_model)$topics

# Calculate the Euclidean distance between documents
similarity_matrix <- proxy::dist(theta, method = "Euclidean")
similarity_matrix <- similarity_matrix / max(similarity_matrix)

pheatmap(similarity_matrix,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         labels_row = doc_labels,
         labels_col = doc_labels,
         main = "Genre Similarity Using Euclidean Distance",
         xlab = "Documents",
         ylab = "Documents",
         color = colorRampPalette(c("Blue", "White"))(100))


```
</center>

## Book Popularity vs Score Standard Deviation

<center>
```{r, warning=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=5}

setwd("..")
review_cnt_std <- readr::read_csv("./data/review_cnt_std.csv")


ggplot(review_cnt_std, aes(x = std_deviation, y = review_count)) +
  geom_point(color = "blue", size = 2) +
  labs(title = "Popularity vs Review Std", x = "Standard Deviation", y = "Review Count")

```
</center>

## Review Length vs Score Given

<center>
```{r, warning=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=5}

ggplot(sent_rat, aes(x = as.factor(rating), y = len_log)) + geom_boxplot() + labs(title = "Distribution of ln(Review Length) by Score Given", x = "Score Given", y = "ln(review length)")

```
</center>

# Killer Plot

## Killer Plot - Catching Fire / Suzanne Collins

<center>
```{r killer, results='hide', fig.width=8, fig.height=5}
setwd('..')
data <- read.csv("data/killer_plot_data.csv")

plot_word_timeseries <- function(data, book_title, smoothing_coefficient=2/3, show_line=FALSE, remove_overlap=FALSE){
  title_data <- data %>%
    filter(title==book_title) %>%
    mutate(date = row_number())
  
  title_checkouts_smoothed <- lowess(title_data$checkouts, f=smoothing_coefficient)
  
  output_plot <- plot_base_graph(title_checkouts_smoothed$x, title_checkouts_smoothed$y)
  
  col <- ifelse(
    title_data$rating < 3, "firebrick",
    ifelse(
      title_data$rating > 3 & title_data$rating < 5, "gold3", "darkgreen"
    )
  )
  grid.text(title_data$word, x=title_checkouts_smoothed$x, title_checkouts_smoothed$y,
            default.units="native", gp=gpar(fontsize=10, col=col), check.overlap=remove_overlap)
  
  if(show_line){
    grid.lines(title_checkouts_smoothed$x, title_checkouts_smoothed$y, default.units="native")  
  }
  
  output_plot
  
}

plot_base_graph <- function(x, y){
  grid.newpage()
  vp <- plotViewport(margins = c(5.1, 4.1, 4.1, 2.1))
  pushViewport(vp)
  pushViewport(dataViewport(x, y))
  grid.rect()
  grid.xaxis()
  grid.yaxis()
}

plot_word_timeseries(data, "Catching fire / Suzanne Collins.", remove_overlap=TRUE)
```
</center>

## Killer Plot - The Girl on the Train: A Novel / Paula Hawkins

<center>
```{r killer2, results='hide', fig.width=8, fig.height=5}
plot_word_timeseries(data, "The Girl on the Train: A Novel")
```
</center>

## Questions?

